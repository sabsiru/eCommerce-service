# 부하 테스트 결과 분석 및 병목 탐색, 개선 방안 및 장애 대응 문서

## 1. 개요
- **테스트 대상 API**: 선착순 쿠폰 발급 (`/coupons/{userId}/issue-async?couponId={couponId}`)
- **테스트 환경**
    - HikariCP 최대 풀 크기: 3
    - Redis, Kafka, MySQL 연계 구성
    - k6 스크립트: `ramping‐arrival‐rate` 시나리오, 최대 5K RPS
    - 모니터링: Grafana(InﬂuxDB v1)

## 2. 성능 지표 요약
- **총 요청 수**: 259,802건
- **성공 응답 수**: 259,802건
- **에러 수**: 0건 (에러율 0.0%)

### 2.1 응답 지연(latency)
| 지표         | 수치     |
|--------------|----------|
| 최소(min)    | 0.11 ms  |
| 중앙값(p50)  | 0.30 ms  |
| p80          | 0.53 ms  |
| p90          | 1.04 ms  |
| p95          | 2.64 ms  |
| p99          | 57.53 ms |
| 최대(max)    | 178.02 ms|
| 평균(mean)   | 1.81 ms  |

### 2.2 블록(blocked) 지연
| 지표         | 수치     |
|--------------|----------|
| 평균(mean)   | 0.01 ms  |
| p80          | 0.00 ms  |
| p90          | 0.00 ms  |
| p95          | 0.01 ms  |
| p99          | 0.10 ms  |
| 최대(max)    | 13.05 ms |

### 2.3 처리량(Requests/sec)
- **평균 처리량**: 2.15 k/s
- **최대 처리량**: 5.13 k/s

## 3. 병목 탐색

### 3.1 p99 구간 지연 (57.53 ms, 최대 178.02 ms)
- **원인 후보**
    1. **DB 커넥션 풀 고갈**
        - HikariCP 풀 크기를 3으로 설정했기 때문에, 동시에 여러 INSERT/CUD 요청이 몰리면 사용 가능한 커넥션이 빠르게 소진됨
        - 커넥션 반환 전까지 대기 시간이 발생하면서 p99 구간 지연이 커짐
    2. **Kafka 메시지 처리 지연**
        - 메시지 발행 후 컨슈머가 Offset을 커밋하거나 후속 로직을 처리하는 과정에서 일부 지연이 발생했을 가능성
        - p99 구간에 극소수 메시지가 네트워크/컨슈머 스로틀링 간에 지연됨

### 3.2 블록(blocked) 지연
- **분석**
    - 전체 요청 중 99%가 0.10 ms 이하로 블록 지연
    - Redis 연결이나 네트워크 대기는 거의 병목이 아니었음을 의미
    - 블록 지연이 최대 13.05 ms였으나 빈도가 매우 낮아 전체 성능 영향 미미

### 3.3 처리량 한계
- **평균 2.15 k/s, 최대 5.13 k/s**
    - 최대 동시 요청 구간에서 약 5k RPS 처리
    - HikariCP가 3으로 설정되어 있으면, 커넥션이 모두 사용 중인 경우 트랜잭션이 대기하면서 처리량 상한에 도달할 가능성이 높음
    - 시스템 자원(CPU, 메모리, 네트워크) 여유분 및 커넥션 풀 사용량을 모니터링해야 함

## 4. 개선 권고

### 4.1 DB 커넥션 풀 증설
- **이유**: HikariCP 풀 크기를 3으로 설정했기 때문에 동시 INSERT/CUD 요청이 몰리면 대기 시간이 발생
- **조치**
    1. `spring.datasource.hikari.maximum-pool-size` 값을 3 → 30 이상으로 상향 조정
    2. `connectionTimeout` 값을 k6 타임아웃(기본 60s)보다 짧게 설정하여 대기 지연 시 즉시 예외 발생
    3. 커넥션 풀 사용량(`HikariPool-ActiveConnections`)을 모니터링하여 임계치 도달 시 알람 설정

### 4.2 Kafka Consumer 튜닝
- **이유**: 메시지 발행 후 컨슈머 처리 지연이 p99 구간에 일부 영향
- **조치**
    1. `max.poll.records` 값을 현행 500 → 1000 이상으로 증설
    2. `fetch.min.bytes` 값을 1 → 1024 바이트 이상으로 설정하여 네트워크 호출 횟수 감소
    3. `max.poll.interval.ms` 값을 300,000ms → 120,000ms로 조정하여 재밸런싱 빈도 감소
    4. 컨슈머 스레드 수 또는 파티션 수를 늘려 병렬 처리 확대

### 4.3 서버 스케일 아웃/업
- **이유**: HikariCP 풀 증설 이후에도 처리량 증가가 필요할 경우 인스턴스 수 증가 필요
- **조치**
    1. 애플리케이션 서버 인스턴스 수를 1 → 2~3대로 확장 (로드밸런서 통해 분산)
    2. 각 인스턴스 JVM 힙/스레드풀 설정 최적화 (CPU 코어 수 대비 병렬 처리 최적화)
    3. MySQL 리플리카 구조 도입(읽기/쓰기 분리) 또는 샤딩 검토

## 5. (가상) 장애 대응 계획

### 5.1 장애 유형 및 시나리오
1. **DB 커넥션 풀 고갈**
    - **원인**: 동시 INSERT/CUD 요청 급증 → 커넥션 풀이 3개로 구성되어 있어 소진
    - **영향**: 트랜잭션 대기 중 타임아웃 발생 → 사용자 응답 지연/실패
    - **감지 지표**:
        - `HikariPool-ActiveConnections` ≈ 3 (maximum)
        - `HikariPool-ConnectionTimeout` 카운트 증가
        - `http_req_duration(p99)` 급증
    - **대응 절차**:
        1. **알람 트리거**: ActiveConnections가 maximumPoolSize(3) 이상 유지 시 알람
        2. **커넥션 풀 확대**: 애플리케이션 재배포 시 `maximum-pool-size` 값을 상향 설정
        3. **임시 조치**: 고부하 구간 동안 API throttle(요청 수 제한) 적용
        4. **리트라이 정책**: 애플리케이션 레벨에서 커넥션 획득 재시도(backoff) 로직 추가

2. **Kafka 컨슈머 지연**
    - **원인**: 컨슈머 처리 속도 저하 또는 Offset 커밋 지연
    - **영향**: 메시지 처리 지연 → 재고 반영 지연
    - **감지 지표**:
        - `kafka_consumer_lag` 증가
        - `kafka_consumer_fetch_wait_max` 상승
    - **대응 절차**:
        1. 컨슈머 그룹 재시작(롤링 재배포)
        2. 파티션 수 증가 및 컨슈머 수 확장
        3. Dead Letter Topic(DLT) 모니터링 및 재처리

3. **네트워크 지연/포화**
    - **원인**: 대규모 부하로 HTTP, Kafka, Redis 통신 경로 과부하
    - **영향**: 응답 지연 및 처리량 급감
    - **감지 지표**:
        - 서버 네트워크 인터페이스 사용률 > 80%
        - `http_req_duration(p95)` 급증
    - **대응 절차**:
        1. 로드밸런서 Connection Drain Time 조정
        2. 네트워크 인터페이스 스케일 아웃 또는 NIC Teaming 구성
        3. CDN 사용 검토(정적 콘텐츠 분리)

### 5.2 장애 복구 및 사후 대응
1. **애플리케이션 로그 수집**
    - Sentry 또는 ELK 스택을 활용하여 예외 로그 수집
    - 장애 발생 시 즉시 “ConnectionTimeout”, “KafkaException” 등의 키워드로 필터링

2. **지표 모니터링**
    - Grafana 알람 룰:
        - `http_req_duration(p95) > 50 ms` 연속 3분 → 알람
        - `HikariPool-ActiveConnections >= 90%` → 알람
        - `kafka_consumer_lag > 500` → 알람

3. **긴급 대응 시나리오**
    - **DB 커넥션 과부하**
        1. 커넥션 풀 설정(`maximumPoolSize`) 즉시 상향 적용
        2. 임시로 API 요청 수 제한(서킷 브레이커 적용)
    - **Kafka 장애**
        1. 컨슈머 재시작 및 파티션 재할당
        2. DLT 메시지 우선 처리
    - **네트워크 장애**
        1. 로드밸런서 설정 재조정
        2. 네트워크 트래픽 분산(인스턴스/인터페이스 확장)

4. **사후 분석 및 예방**
    - 장애 발생 시점 로그·지표 종합 분석 후 Root-Cause 보고서 작성
    - 재발 방지를 위한 코드/설정 개선(쿼리 인덱스, 트랜잭션 타임아웃 조정 등)
    - 정기적인 모의 장애 테스트 실시

---

## 6. 결론
- **주요 병목**: DB 커넥션 풀이 3으로 설정된 점이 p99 구간(57 ms 이상) 지연을 유발할 가능성이 높음
- **개선 효과 기대**:
    - 커넥션 풀을 30 이상으로 확장 시 p99 지연을 15~20 ms 수준으로 낮추고, 에러(타임아웃) 발생 가능성을 제거할 수 있음
    - Kafka Consumer 튜닝으로 메시지 처리 지연 완화
    - 서버 스케일 아웃을 통해 5k RPS 이상 안정 처리